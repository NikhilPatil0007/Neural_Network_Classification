# -*- coding: utf-8 -*-
"""Linear_Neural_Network_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZbD5a1GIGnwMdaWTRXP7G0AoM3R0NFso

"Toy Datasets -  We are using this dataset to practice Fundaments"
"""

from sklearn.datasets import make_circles

#Make 1000 samples
n_samples = 1000

#Create Circles
X, y = make_circles(n_samples,
                    noise = 0.03,
                    random_state = 42)

len(X), len(y)

"""It has two features of X and One label of y"""

print("X: ",X[:5])
print("Y: ",y[:5])

#Make dataframe of circle data
import pandas as pd
circles = pd.DataFrame({"X1" : X[:, 0],
                        "X2" :X [:, 1],
                        "label" : y})
circles

circles.label.value_counts()

#Visualize
import matplotlib.pyplot as plt
plt.scatter(x=X[: , 0],
            y=X[:,1],
            c = y,
            cmap = plt.cm.RdYlBu)
plt.show()

X.shape, y.shape

#Turn data into tensors
import torch
from torch import tensor
X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

#torch.manual_seed(42)

#Splittin the dataset into training and testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size = 0.2,
                                                    random_state = 42)

len(X_train), len(X_test), len(y_train), len(y_test)

"""#2 Building a Model
let's build to model to classify our blue and red dots
1. Construct a Model (By subclassing 'nn.module')
2. Define a loss function and optimizer
3. Create a training and testing loop
"""

import torch
from torch import nn

X_train

# 1. Construct a model that subclasses nn.module
class CircleModelV0(nn.Module):
  def __init__(self):
    super().__init__()
    #Create  2 nn.Linear layers capable of handling the shapes of  our data
    self.layer_1 = nn.Linear(in_features = 2,
                             out_features = 5)
    self.layer_2 = nn.Linear(in_features = 5,
                             out_features = 1)
  def forward(self, x):
    return self.layer_2(self.layer_1(x)) #x -> layer1 -> layer2\

model_0 = CircleModelV0()
model_0

# #Using Sequential we are goint to replicate above Code
# model_1 = nn.Sequential(
#     nn.linear(in_features = 2,
#               out_features = 5),
#     nn.Linear(in_features = 5,
#               out_features = 1),
#     nn.Sigmoid()
#         )
# model_1

model_0.state_dict()

#Make Predictions
with torch.inference_mode():
  untrained_preds = model_0(X_test.type(torch.float))
untrained_preds.shape
print("First five Untrain preds",untrained_preds[:5])
print("First five Untrain labels",y_test[:5])

# Setup the loss function
loss_fn = nn.BCEWithLogitsLoss()

optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.1)

# Calculate accuracy (a classification metric)
def accuracy_fn(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal
    acc = (correct / len(y_pred)) * 100
    return acc

# View the frist 5 outputs of the forward pass on the test data
y_logits = model_0(X_test)[:5]
y_logits

# Use sigmoid on model logits
y_pred_probs = torch.sigmoid(y_logits)
y_pred_probs

# Find the predicted labels (round the prediction probabilities)
y_preds = torch.round(y_pred_probs)

# In full
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test)[:5]))

# Check for equality
print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))

# Get rid of extra dimension
y_preds.squeeze()

y_test[:5]

"""Train a Model
TO train our model, we're going to need to build a training loop with the following steps:
1. Forward Pass
2. Calculate the Loss
3. Optimizer Zero grad
4. Loss Backward(Backpropagation)
5. Optimizer Step
"""

torch.manual_seed(42)

epochs = 1000

for epoch in range(epochs):
  #1. Training
  model_0.train()
  #2. Forward Pass
  y_logits = model_0(X_train.type(torch.float)).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  #3. Loss Function and Accuracy
  loss = loss_fn(y_logits,
                 y_train)
  acc = accuracy_fn(y_true = y_train,
                    y_pred = y_pred)
  #4. Optimizer Zero Grad
  optimizer.zero_grad()
  #5. Backpropagation
  loss.backward()
  #6. Optimizer Step
  optimizer.step()

  ##Testing
  model_0.eval()
  with torch.inference_mode():
    #forward Pass
    test_logits = model_0(X_test.type(torch.float)).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))
    #loss
    test_loss = loss_fn(test_logits,
                        y_test)
    test_acc = accuracy_fn(y_true = y_test,
                           y_pred = test_pred)
    #print out whats happening
    if epoch % 100 == 0:
      print(f"Epoch: {epoch} | Loss: {loss:.5f} | Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Accuracy: {test_acc:.2f}%")

## Make predictions and evaluate the model
# Plot linear data or training and test and predictions (optional)
import torch
import matplotlib.pyplot as plt
import numpy as np
def plot_predictions(
    train_data, train_labels, test_data, test_labels, predictions=None
):
    """
  Plots linear training data and test data and compares predictions.
  """
    plt.figure(figsize=(10, 7))

    # Plot training data in blue
    plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")

    # Plot test data in green
    plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data")

    if predictions is not None:
        # Plot the predictions in red (predictions were made on the test data)
        plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")

    # Show the legend
    plt.legend(prop={"size": 14})
def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):

    # Put everything to CPU (works better with NumPy + Matplotlib)
    model.to("cpu")
    X, y = X.to("cpu"), y.to("cpu")

    # Setup prediction boundaries and grid
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))

    # Make features
    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()

    # Make predictions
    model_0.eval()
    with torch.inference_mode():
        y_logits = model(X_to_pred_on)

    # Test for multi-class or binary and adjust logits to prediction labels
    if len(torch.unique(y)) > 2:
        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class
    else:
        y_pred = torch.round(torch.sigmoid(y_logits))  # binary

    # Reshape preds and plot
    y_pred = y_pred.reshape(xx.shape).detach().numpy()
    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.title("Train")
plot_decision_boundary(model_0, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Test")
plot_decision_boundary(model_0, X_test, y_test)

class CircleModelV1(nn.Module):
  def __init__(self):
    super().__init__()
    #Create  2 nn.Linear layers capable of handling the shapes of  our data
    self.layer_1 = nn.Linear(in_features = 2,
                             out_features = 10)
    self.layer_2 = nn.Linear(in_features = 10,
                             out_features = 10)
    self.layer_3 = nn.Linear(in_features = 10,
                             out_features = 1)
  def forward(self, x):
    z = self.layer_1(x)
    z = self.layer_2(z)
    z = self.layer_3(z)
    return self.layer_3(self.layer_2(self.layer_1(x))) #x -> layer1 -> layer2\

model_1 = CircleModelV1()
model_1

# Create loss function
loss_fn = nn.BCEWithLogitsLoss()

# Create optimizer
optimizer = torch.optim.SGD(params=model_1.parameters(),
                            lr=0.1)

# Write a training and evaluation loop
torch.manual_seed(42)

epochs = 1000
for epoch in range(epochs):
  model_1.train()
  y_logits = model_1(X_train.type(torch.float)).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))
  loss = loss_fn(y_logits,
                 y_train)
  acc = accuracy_fn(y_true = y_train,
                    y_pred = y_pred)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

   ##Testing
  model_1.eval()
  with torch.inference_mode():
    #forward Pass
    test_logits = model_1(X_test.type(torch.float)).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))
    #loss
    test_loss = loss_fn(test_logits,
                        y_test)
    test_acc = accuracy_fn(y_true = y_test,
                           y_pred = test_pred)
    #print out whats happening
    if epoch % 100 == 0:
      print(f"Epoch: {epoch} | Loss: {loss:.5f} | Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f} | Test Accuracy: {test_acc:.2f}%")

